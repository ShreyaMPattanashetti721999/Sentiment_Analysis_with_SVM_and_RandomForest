{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeBr_NhGXtzC"
   },
   "source": [
    "# NLP Assignment 1 (40% of grade): Sentiment Analysis from Tweets\n",
    "\n",
    "This coursework will involve you implementing functions for a text classifier, which you will train to identify the **sentiment expressed in a text** in a dataset of approx. 27,000 entries, which will be split into a 80%/20% training/test split.\n",
    "\n",
    "In this template you are given the basis for that implementation, though some of the functions are missing, which you have to fill in.\n",
    "\n",
    "Follow the instructions file **NLP_Assignment_1_Instructions.pdf** for details of each question - the outline of what needs to be achieved for each question is as below.\n",
    "\n",
    "You must submit all **ipython notebooks and extra resources you need to run the code if you've added them** in the code submission, and a **2 page report (pdf)** in the report submission on QMPlus where you report your methods and findings according to the instructions file for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRpZVRJpXtzH"
   },
   "source": [
    "# Questions 5: Optimising pre-processing and feature extraction (30 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y0d-wll7XtzE"
   },
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLcLcDXOET4i",
    "outputId": "fe022cf4-9c80-496b-c9b3-54aa0e77d876"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmtYDspGEav2",
    "outputId": "7d23c876-9952-4d7c-a7a8-053777321db0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p0GQay-hXtzE"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"Id\":  # skip header\n",
    "                continue\n",
    "            (label, text) = parse_data_line(line)\n",
    "            raw_data.append((text, label))\n",
    "\n",
    "def split_and_preprocess_data(percentage):\n",
    "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
    "    and performs the preprocessing.\"\"\"\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int((percentage * num_samples))\n",
    "    for (text, label) in raw_data[:num_training_samples]:\n",
    "        train_data.append((to_feature_vector(pre_process(text)),label))\n",
    "    for (text, label) in raw_data[num_training_samples:]:\n",
    "        test_data.append((to_feature_vector(pre_process(text)),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AozUVe0WXtzF"
   },
   "outputs": [],
   "source": [
    "def parse_data_line(data_line):\n",
    "    # Should return a tuple of the label as just positive or negative and the statement\n",
    "    # e.g. (label, statement)\n",
    "    label, text = data_line[1], data_line[2]\n",
    "    return (label, text)\n",
    "    #return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f6qU4pBHXtzF"
   },
   "outputs": [],
   "source": [
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "    # Tokenization and lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IZoO6teZXtzF"
   },
   "outputs": [],
   "source": [
    "global_feature_dict = {} # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    feature_vector = {}\n",
    "    for token in tokens:\n",
    "        feature_vector[token] = 1  # Binary feature, 1 if the feature is present, 0 if it's not\n",
    "        global_feature_dict[token] = global_feature_dict.get(token, 0) + 1  # Increment count in the global feature dictionary\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jD4L4eDCXtzG"
   },
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_classifier(data):\n",
    "    print(\"Training Classifier with Random Forest...\")\n",
    "    pipeline = Pipeline([('rfc', RandomForestClassifier())])  # Using RandomForestClassifier as the classifier\n",
    "    return SklearnClassifier(pipeline).train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HkwgrL7-XtzG"
   },
   "outputs": [],
   "source": [
    "#solution\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "def cross_validate(dataset, folds):\n",
    "    results = []\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(dataset):\n",
    "        train_data = [dataset[i] for i in train_index]\n",
    "        test_data = [dataset[i] for i in test_index]\n",
    "\n",
    "        classifier = train_classifier(train_data)\n",
    "        predicted_labels = predict_labels([data[0] for data in test_data], classifier)\n",
    "        true_labels = [data[1] for data in test_data]\n",
    "\n",
    "        report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "        results.append(report)\n",
    "\n",
    "    # Calculate average scores over all folds\n",
    "    avg_results = {\n",
    "        'precision': np.mean([result['macro avg']['precision'] for result in results]),\n",
    "        'recall': np.mean([result['macro avg']['recall'] for result in results]),\n",
    "        'f1-score': np.mean([result['macro avg']['f1-score'] for result in results]),\n",
    "        'accuracy': np.mean([result['accuracy'] for result in results])\n",
    "    }\n",
    "\n",
    "    return avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R-EkYY6dXtzG"
   },
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
    "    return classifier.classify_many(samples)\n",
    "\n",
    "def predict_label_from_raw(sample, classifier):\n",
    "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
    "\n",
    "    return classifier.classify(to_feature_vector(pre_process(sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJFMynGVXtzG",
    "outputId": "e3d342b6-c5bf-4c8f-c3b3-319f463be13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 33540 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 33540 rawData, 26832 trainData, 6708 testData\n",
      "Training Samples: \n",
      "26832\n",
      "Features: \n",
      "58934\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "raw_data = []          # the filtered data from the dataset file\n",
    "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "\n",
    "# references to the data files\n",
    "data_file_path = 'sentiment-dataset.tsv'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "load_data(data_file_path)\n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7gordEiXtzG",
    "outputId": "b1d13e4a-7959-4e93-837a-7bd5c4a1431a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier with Random Forest...\n",
      "Training Classifier with Random Forest...\n",
      "Training Classifier with Random Forest...\n",
      "Training Classifier with Random Forest...\n",
      "Training Classifier with Random Forest...\n",
      "Training Classifier with Random Forest...\n",
      "Training Classifier with Random Forest...\n",
      "Training Classifier with Random Forest...\n",
      "Training Classifier with Random Forest...\n",
      "Training Classifier with Random Forest...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.838961586787528,\n",
       " 'recall': 0.7821519872856413,\n",
       " 'f1-score': 0.7993711744713882,\n",
       " 'accuracy': 0.8319170546127769}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(train_data, 10)  #Â will work and output overall performance of p, r, f-score when cv implemented"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
